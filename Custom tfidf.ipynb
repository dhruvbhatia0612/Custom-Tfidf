{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1 : Custom Implementation of TFIDF Vectorizer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this']\n",
      "The custom implemented idf values are :  [1.916290731874155, 1.2231435513142097, 1.5108256237659907, 1.0, 1.916290731874155, 1.916290731874155, 1.0, 1.916290731874155, 1.0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from collections import Counter\n",
    "from scipy.sparse import csr_matrix\n",
    "import numpy as np\n",
    "import math as m\n",
    "\n",
    "from tqdm import tqdm \n",
    "\n",
    "\n",
    "def fit(dataset):    \n",
    "    unique_words = set() \n",
    "    \n",
    "    if isinstance(dataset, (list,)):\n",
    "        for row in dataset: \n",
    "            for word in row.split(\" \"): \n",
    "                if len(word) < 2:\n",
    "                    continue\n",
    "                unique_words.add(word)\n",
    "        unique_words = sorted(list(unique_words))\n",
    "        vocab = {j:i for i,j in enumerate(unique_words)}\n",
    "        #Creating a list of number of documents each unique word is :\n",
    "        doc_num=[]\n",
    "        for i in range (0,len(unique_words)):\n",
    "            count=0\n",
    "            for j in range (0,len(dataset)):\n",
    "                for k in range (0,len(dataset[j].split())):\n",
    "                    \n",
    "                    if unique_words[i]==dataset[j].split()[k]:\n",
    "                        count=count+1\n",
    "                        break\n",
    "            doc_num.append(count)\n",
    "                        \n",
    "                  \n",
    "                    \n",
    "        return vocab,doc_num\n",
    "    else:\n",
    "        print(\"you need to pass list of sentance\")\n",
    "        \n",
    "        \n",
    "def transform(dataset,vocab,num):\n",
    "    rows = []\n",
    "    columns = []\n",
    "    values = []\n",
    "    #Computing idf values for each unique word\n",
    "    idf=[]\n",
    "    nume=1+len(dataset)\n",
    "    for g in range (0,len(num)):\n",
    "        \n",
    "        den=1+num[g]\n",
    "        idf_val=1+m.log(nume/den)\n",
    "        idf.append(idf_val) \n",
    "    print(\"The custom implemented idf values are : \",idf)\n",
    "    if isinstance(dataset, (list,)):\n",
    "        \n",
    "        for idx, row in enumerate(tqdm(dataset)): \n",
    "            word_freq = dict(Counter(row.split()))\n",
    "            \n",
    "            for word, freq in word_freq.items(): \n",
    "                \n",
    "                if len(word) < 2:\n",
    "                    continue\n",
    "                #Computing tf value\n",
    "                tf=freq/len(dataset[idx].split())     \n",
    "                col_index = vocab.get(word, -1) \n",
    "                if col_index !=-1:\n",
    "                    rows.append(idx)\n",
    "                    columns.append(col_index)\n",
    "                    #getting Tf*idf\n",
    "                    values.append(tf*idf[col_index])\n",
    "                    \n",
    "           \n",
    "        return csr_matrix((values, (rows,columns)), shape=(len(dataset),len(vocab)))\n",
    "    else:\n",
    "        print(\"you need to pass list of strings\")\n",
    "dataset = [\n",
    "     'this is the first document',\n",
    "     'this document is the second document',\n",
    "     'and this is the third one',\n",
    "     'is this the first document',\n",
    "]\n",
    "from sklearn import preprocessing\n",
    "vocab,num=fit(dataset)\n",
    "print(list(vocab.keys()))\n",
    "X=transform(dataset,vocab,num).toarray()\n",
    "Y=csr_matrix(preprocessing.normalize(X,norm='l2'))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The feature names and vocab have the same unique words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "vectorizer.fit(dataset)\n",
    "skl_output = vectorizer.transform(dataset)\n",
    "\n",
    "print(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The custom idf values and TfidfVectorizer values are same ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.91629073 1.22314355 1.51082562 1.         1.91629073 1.91629073\n",
      " 1.         1.91629073 1.        ]\n"
     ]
    }
   ],
   "source": [
    "print(vectorizer.idf_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The tfidf values for a particular document(here doc=0) is obtained and verified "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 1)\t0.4697913855799205\n",
      "  (0, 2)\t0.580285823684436\n",
      "  (0, 3)\t0.3840852409148149\n",
      "  (0, 6)\t0.3840852409148149\n",
      "  (0, 8)\t0.3840852409148149\n"
     ]
    }
   ],
   "source": [
    "print(Y[0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 8)\t0.38408524091481483\n",
      "  (0, 6)\t0.38408524091481483\n",
      "  (0, 3)\t0.38408524091481483\n",
      "  (0, 2)\t0.5802858236844359\n",
      "  (0, 1)\t0.46979138557992045\n"
     ]
    }
   ],
   "source": [
    "print(skl_output[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Printing dense matrix for a particular doc ( here doc=0). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.46979139 0.58028582 0.38408524 0.         0.\n",
      "  0.38408524 0.         0.38408524]]\n"
     ]
    }
   ],
   "source": [
    "print(Y[0,:].toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2 :  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 746/746 [00:00<00:00, 12896.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872]\n",
      "{'aailiyah': 0, 'abandoned': 1, 'abroad': 2, 'abstruse': 3, 'academy': 4, 'accents': 5, 'accessible': 6, 'acclaimed': 7, 'accolades': 8, 'accurate': 9, 'accurately': 10, 'achille': 11, 'ackerman': 12, 'actions': 13, 'adams': 14, 'add': 15, 'added': 16, 'admins': 17, 'admiration': 18, 'admitted': 19, 'adrift': 20, 'adventure': 21, 'aesthetically': 22, 'affected': 23, 'affleck': 24, 'afternoon': 25, 'aged': 26, 'ages': 27, 'agree': 28, 'agreed': 29, 'aimless': 30, 'aired': 31, 'akasha': 32, 'akin': 33, 'alert': 34, 'alike': 35, 'allison': 36, 'allow': 37, 'allowing': 38, 'alongside': 39, 'amateurish': 40, 'amaze': 41, 'amazed': 42, 'amazingly': 43, 'amusing': 44, 'amust': 45, 'anatomist': 46, 'angel': 47, 'angela': 48, 'angelina': 49}\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from collections import Counter\n",
    "from scipy.sparse import csr_matrix\n",
    "import numpy as np\n",
    "import math as m\n",
    "\n",
    "from tqdm import tqdm \n",
    "\n",
    "\n",
    "def fit(dataset):    \n",
    "    unique_words = set() \n",
    "    \n",
    "    if isinstance(dataset, (list,)):\n",
    "        for row in dataset: \n",
    "            for word in row.split(\" \"): \n",
    "                if len(word) < 2:\n",
    "                    continue\n",
    "                unique_words.add(word)\n",
    "        unique_words = sorted(list(unique_words))\n",
    "        #Computing for each unique word : in how many documents does the word occur\n",
    "        doc_num=[]\n",
    "        for i in range (0,len(unique_words)):\n",
    "            count=0\n",
    "            for j in range (0,len(dataset)):\n",
    "                for k in range (0,len(dataset[j].split())):\n",
    "                    \n",
    "                    if unique_words[i]==dataset[j].split()[k]:\n",
    "                        count=count+1\n",
    "                        break\n",
    "            doc_num.append(count)\n",
    "        #Computing idf values\n",
    "        idf=[]\n",
    "        nume=1+len(dataset)\n",
    "        for g in range (0,len(doc_num)):\n",
    "            den=1+doc_num[g]\n",
    "            idf_val=1+m.log(nume/den)\n",
    "            idf.append(idf_val)\n",
    "        #making a dictionary of unique words with idf values\n",
    "        vocab = {j:idf[i] for i,j in enumerate(unique_words)}\n",
    "        #calculating words with highest idf value\n",
    "        d=Counter(vocab)\n",
    "        #getting top 50 idf values\n",
    "        voc=dict(d.most_common(50))\n",
    "        idf=sorted(idf)\n",
    "        idf.reverse()\n",
    "        idf=idf[0:50]\n",
    "        word=voc.keys()\n",
    "        #creating a dictionary of top 50 idf words for thetransform function\n",
    "        voc1={k:l for l,k in enumerate(word)}\n",
    "        \n",
    "        \n",
    "        \n",
    "        return voc1,doc_num,idf\n",
    "    else:\n",
    "        print(\"you need to pass list of sentance\")\n",
    "        \n",
    "        \n",
    "\n",
    "def transform(dataset,vocab,num,idf):\n",
    "    rows = []\n",
    "    columns = []\n",
    "    values = []\n",
    "    \n",
    "    if isinstance(dataset, (list,)):\n",
    "        \n",
    "        for idx, row in enumerate(tqdm(dataset)): \n",
    "            word_freq = dict(Counter(row.split()))\n",
    "            \n",
    "            for word, freq in word_freq.items(): \n",
    "                \n",
    "                if len(word) < 2:\n",
    "                    continue\n",
    "                tf=freq/len(dataset[idx].split())     \n",
    "                col_index = vocab.get(word, -1) \n",
    "                if col_index !=-1:\n",
    "                    rows.append(idx)\n",
    "                    columns.append(col_index)\n",
    "                    values.append(tf*idf[col_index])\n",
    "                    \n",
    "           \n",
    "        return csr_matrix((values, (rows,columns)), shape=(len(dataset),len(vocab)))\n",
    "    else:\n",
    "        print(\"you need to pass list of strings\")\n",
    "import pickle\n",
    "with open('cleaned_strings', 'rb') as f:\n",
    "    dataset = pickle.load(f)\n",
    "\n",
    "vocab,num,idf=fit(dataset)\n",
    "X=transform(dataset,vocab,num,idf)\n",
    "Y=csr_matrix(preprocessing.normalize(X,norm='l2'))\n",
    "#top 50 idf values \n",
    "print(idf)\n",
    "#top 50 words with idf values\n",
    "print(vocab)\n",
    "print(Y[0,:].toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 30)\t1.0\n"
     ]
    }
   ],
   "source": [
    "print(Y[0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
